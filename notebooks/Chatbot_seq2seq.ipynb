{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42aff312",
   "metadata": {},
   "source": [
    "# Seq2Seq (Chatbot Building)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a9314",
   "metadata": {},
   "source": [
    "Part of **#30DaysOfBasics**, Lets do SEq2Seq (Encoder-Decoder) modelling in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42c72b",
   "metadata": {},
   "source": [
    "Training Data: Cornell Movies dialogs (https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "\n",
    "Referencev tutorial: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761aeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import csv \n",
    "import codecs\n",
    "import itertools\n",
    "import json\n",
    "import collections\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1b5d7",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e421d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_movie_convo = '../data/movie_conversations.txt'\n",
    "corpus_movie_lines = '../data/movie_lines.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a5a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_movie_convo, 'r', encoding='iso-8859-1') as f:\n",
    "    movie_convo = f.readlines()\n",
    "    \n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as f:\n",
    "    movie_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6191fe91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
      "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
      "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n"
     ]
    }
   ],
   "source": [
    "for line in movie_lines[:8]:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dead0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fields = ['lineID', 'characterID', 'movieID', 'character', 'text']\n",
    "\n",
    "lines = {}\n",
    "\n",
    "with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split('+++$+++')\n",
    "        tempLineDict = {}\n",
    "        \n",
    "        for idx, field in enumerate(line_fields):\n",
    "            tempLineDict[field] = values[idx].strip()\n",
    "        lines[tempLineDict['lineID']] = tempLineDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df1304df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1045': {'lineID': 'L1045',\n",
       "  'characterID': 'u0',\n",
       "  'movieID': 'm0',\n",
       "  'character': 'BIANCA',\n",
       "  'text': 'They do not!'},\n",
       " 'L1044': {'lineID': 'L1044',\n",
       "  'characterID': 'u2',\n",
       "  'movieID': 'm0',\n",
       "  'character': 'CAMERON',\n",
       "  'text': 'They do to!'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(itertools.islice(lines.items(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed51508",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_fields = ['character_1ID', 'character_2ID', 'movieID', 'utteranceIDs']\n",
    "\n",
    "conversations = []\n",
    "\n",
    "with open(corpus_movie_convo, 'r', encoding='iso-8859-1') as f:\n",
    "    for line in f:\n",
    "        values = line.split('+++$+++')\n",
    "        \n",
    "        tempConvDict = {}\n",
    "        \n",
    "        for idx, field in enumerate(conv_fields):\n",
    "            tempConvDict[field] = values[idx].strip()\n",
    "        \n",
    "        tempConvDict['lines'] = []\n",
    "        lineIDs = eval(tempConvDict['utteranceIDs'])\n",
    "        \n",
    "        for lineID in lineIDs:\n",
    "            tempConvDict['lines'].append(lines[lineID])\n",
    "        conversations.append(tempConvDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bc57eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character_1ID': 'u0',\n",
       " 'character_2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L194', 'L195', 'L196', 'L197']\",\n",
       " 'lines': [{'lineID': 'L194',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'},\n",
       "  {'lineID': 'L195',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Well, I thought we'd start with pronunciation, if that's okay with you.\"},\n",
       "  {'lineID': 'L196',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': 'Not the hacking and gagging and spitting part.  Please.'},\n",
       "  {'lineID': 'L197',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d1707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "for conversation in conversations:\n",
    "    \n",
    "    for idx in range(len(conversation['lines']) - 1):\n",
    "        input_line = conversation['lines'][idx]['text'].strip()\n",
    "        target_line = conversation['lines'][idx+1]['text'].strip()\n",
    "        \n",
    "        if input_line and target_line:\n",
    "            qa_pairs.append([input_line, target_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26de7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\"], [\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.']]\n",
      "****************************************************************************************************\n",
      "221282\n"
     ]
    }
   ],
   "source": [
    "print(qa_pairs[:2])\n",
    "print('*'*100)\n",
    "print(len(qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f4dce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimeter = str(codecs.decode('\\t', 'unicode_escape'))\n",
    "\n",
    "with open('../data/formatted_movie_convo_pairs.txt', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter = delimeter)\n",
    "    \n",
    "    for pair in qa_pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a521c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class CustomVocabulary:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {PAD_TOKEN:'PAD', SOS_TOKEN:'SOS', EOS_TOKEN:'EOS'}\n",
    "        self.n_words = 3\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            self.add_word(word.strip())\n",
    "            \n",
    "    def trim_vocab(self, min_freq_count):\n",
    "        words_to_keep = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_freq_count:\n",
    "                words_to_keep.append(k)\n",
    "        \n",
    "        #reinitializa the vocab\n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.idx2word = {PAD_TOKEN:'PAD', SOS_TOKEN:'SOS', EOS_TOKEN:'EOS'}\n",
    "        self.n_words = 3\n",
    "        \n",
    "        for word in words_to_keep:\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb8860b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    \n",
    "    s = re.sub(r'([.!?])', r'\\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "468f9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pairs = [[normalizeString(pair[0]), normalizeString(pair[1])] for pair in qa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af2e365",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = CustomVocabulary('cornell movie corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e4faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split()) < MAX_LEN and len(p[1].split()) < MAX_LEN\n",
    "\n",
    "def filter_all_pairs(all_pairs):\n",
    "    return [pair for pair in all_pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cb6317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trimming, number of pairs:  221282\n",
      "After trimming, number of pairs:  91004\n"
     ]
    }
   ],
   "source": [
    "print('Before trimming, number of pairs: ', len(qa_pairs))\n",
    "filtered_qa_pairs = filter_all_pairs(qa_pairs)\n",
    "print('After trimming, number of pairs: ', len(filtered_qa_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a485a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of words in vocab:  63242\n"
     ]
    }
   ],
   "source": [
    "#Populating the vocab\n",
    "\n",
    "for pair in filtered_qa_pairs:\n",
    "#     print(pair)\n",
    "    voc.add_sentence(pair[0])\n",
    "    voc.add_sentence(pair[1])\n",
    "    \n",
    "print('Total num of words in vocab: ', voc.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c981204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_rare_words(vocabulary, pairs, min_count):\n",
    "    \n",
    "#     print(vocabulary.n_words)\n",
    "    vocabulary.trim_vocab(min_count)\n",
    "#     print(vocabulary.n_words)   \n",
    "    \n",
    "    #keeping pairs who have vocab words\n",
    "    keep_pairs = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "#         print(pair)\n",
    "        input_seq = pair[0]\n",
    "        output_seq = pair[1]\n",
    "        \n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        \n",
    "        for word in input_seq.split(' '):\n",
    "            if word not in vocabulary.word2idx:\n",
    "                keep_input = False\n",
    "                break\n",
    "        for word in output_seq.split(' '):\n",
    "            if word not in vocabulary.word2idx:\n",
    "                keep_output = False\n",
    "                break\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "            \n",
    "    print('No of pairs before trimming: ', len(pairs))\n",
    "    print('No of pairs after trimmin: ', len(keep_pairs))\n",
    "    \n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad04d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of pairs before trimming:  91004\n",
      "No of pairs after trimmin:  46025\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3\n",
    "trimmed_pairs = trim_rare_words(voc, filtered_qa_pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57306cf6",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "751b2f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_indexing(vocab, sentence):\n",
    "    return [vocab.word2idx[word] for word in sentence.split()] + [EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd9de3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(iterList, fillvalue=0):\n",
    "    return list(itertools.zip_longest(*iterList, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1bf6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMask(list_of_list_of_indexes):\n",
    "    \n",
    "    mask = []\n",
    "    for idx, seq in enumerate(list_of_list_of_indexes):\n",
    "        mask.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_TOKEN:\n",
    "                mask[idx].append(0)\n",
    "            else:\n",
    "                mask[idx].append(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baf46611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(list_of_input, vocab):\n",
    "    indexed_batch = [sentence_indexing(vocab, sentence) for sentence in list_of_input]\n",
    "    lengths_tensor = torch.tensor([len(indexed_list) for indexed_list in indexed_batch])\n",
    "    padList = zero_padding(indexed_batch)\n",
    "    padded_tensor = torch.LongTensor(padList)\n",
    "    return padded_tensor, lengths_tensor\n",
    "\n",
    "def encode_output(list_of_output, vocab):\n",
    "    indexed_batch = [sentence_indexing(vocab, sentence) for sentence in list_of_output]\n",
    "    max_target_len = max([len(indexed_list) for indexed_list in indexed_batch])\n",
    "    padList = zero_padding(indexed_batch)\n",
    "    mask = binaryMask(padList)\n",
    "    mask_tensor = torch.ByteTensor(mask)\n",
    "    padded_tensor = torch.LongTensor(padList)\n",
    "    return padded_tensor, mask_tensor, max_target_len\n",
    "\n",
    "def batch2TrainData(vocab, pair_batch):\n",
    "    \n",
    "    #sorting the questions in descending order\n",
    "    pair_batch.sort(key= lambda x: len(x[0].split()), reverse=True)\n",
    "    \n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    \n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "        \n",
    "    ip, lenghts = encode_input(input_batch, vocab)\n",
    "    op, mask, max_tar_len = encode_output(output_batch, vocab)\n",
    "    \n",
    "    return ip, lenghts, op, mask, max_tar_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "158e8863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input seq:  tensor([[ 950,  624,   15,  455,  111],\n",
      "        [  50,   20, 1008,  119,    2],\n",
      "        [  95,  191, 2845,  456,    0],\n",
      "        [ 393, 5248, 9349,    2,    0],\n",
      "        [   2,    2,    2,    0,    0]])\n",
      "--------------------------------------------------\n",
      "Input seq Shape:  torch.Size([5, 5])\n",
      "Lengths:  tensor([5, 5, 5, 4, 2])\n",
      "****************************************************************************************************\n",
      "Output seq:  tensor([[  294,   270,   117, 14763,   205],\n",
      "        [ 9499,  1357,   745,     2,    67],\n",
      "        [  656,  5249,    67,     0,   277],\n",
      "        [ 9500,     2,  4901,     0,   384],\n",
      "        [    2,     0,     2,     0,     2]])\n",
      "--------------------------------------------------\n",
      "Output seq Shape:  torch.Size([5, 5])\n",
      "Mask:  tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 1, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1]], dtype=torch.uint8)\n",
      "--------------------------------------------------\n",
      "Mask Shape:  torch.Size([5, 5])\n",
      "Max target Length:  5\n"
     ]
    }
   ],
   "source": [
    "#validating data preparation steps\n",
    "\n",
    "small_batch_size = 5\n",
    "\n",
    "batches = batch2TrainData(voc, [random.choice(trimmed_pairs) for _ in range(small_batch_size)])\n",
    "\n",
    "print('Input seq: ', batches[0])\n",
    "print('-'*50)\n",
    "print('Input seq Shape: ', batches[0].shape)\n",
    "print('Lengths: ', batches[1])\n",
    "print('*'*100)\n",
    "print('Output seq: ', batches[2])\n",
    "print('-'*50)\n",
    "print('Output seq Shape: ', batches[2].shape)\n",
    "print('Mask: ', batches[3])\n",
    "print('-'*50)\n",
    "print('Mask Shape: ', batches[3].shape)\n",
    "print('Max target Length: ', batches[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b0ebf",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "546d203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0728e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        \n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        #input_size for our GRU encoder is equal to hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, \n",
    "                          n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        #input_seq: (max_length, batch_size)\n",
    "        #input_lengths:  batch_size\n",
    "        #hidden_state: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        #pack_padded_sequence: takesn input of padded_batch and their corresponding variable lengths\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        \n",
    "        #output_shape: (seq_len, batch_size, num_directions * hidden_size)\n",
    "        #hidden_shape: (n_layers * num_directions, batch_size, hidden_size)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        #only if bidirectional==True then summing the outputs of both gru\n",
    "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a1f9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, method):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.method = method\n",
    "        \n",
    "    def dot_product(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        atten_weights = self.dot_product(hidden, encoder_outputs)\n",
    "        atten_weights = atten_weights.t()\n",
    "        \n",
    "        #return (batch_size, 1, max_length)\n",
    "        return nn.functional.softmax(atten_weights, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "743bdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoderAttn(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention_method, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(GRUDecoderAttn, self).__init__()\n",
    "        self.attention_method = attention_method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.atten = Attention(attention_method, hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        \n",
    "        #input_seq: one time step of input seq batch, (1, batch_size)\n",
    "        #last_hidden: final hidden states of GRU, (n_layers * n_directions, batch_size, hidden_size)\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        #gru_output: (1, batch_size, n_directions * hidden_size)\n",
    "        gru_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        attn_weights = self.atten(gru_output, encoder_outputs)\n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        \n",
    "        gru_output = gru_output.squeeze(0)\n",
    "        \n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        concat_input = torch.cat((gru_output, context), 1)\n",
    "        \n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "        out = self.out(concat_output)\n",
    "        \n",
    "        out = nn.functional.softmax(out, dim=1)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fb87dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(decoder_output, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    target = target.view(-1,1)\n",
    "    \n",
    "    gathered_tensor = torch.gather(decoder_output, 1, target)\n",
    "    crossEntropy = -torch.log(gathered_tensor)\n",
    "    loss = crossEntropy.masked_select(mask)\n",
    "    \n",
    "    return loss.mean(), nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77650418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output is of torch.Size([8, 5, 512]) and Encoder hidden is of torch.Size([4, 5, 512]) shapes\n",
      "****************************************************************************************************\n",
      "Initial Decoder input is of torch.Size([1, 5]) shapes\n",
      "Initial Decoder hidden state is of torch.Size([2, 5, 512]) shape\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<--Inside the one time step of GRU-->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[0]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[0]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.859118461608887 and nTotal: 5\n",
      "print_losses: [49.295592308044434]\n",
      "Returned loss:  9.859118461608887\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[1]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[1]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.859922409057617 and nTotal: 5\n",
      "print_losses: [49.295592308044434, 49.299612045288086]\n",
      "Returned loss:  9.859520435333252\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[2]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[2]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.867183685302734 and nTotal: 5\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367]\n",
      "Returned loss:  9.862074851989746\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[3]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[3]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.842935562133789 and nTotal: 4\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367, 39.371742248535156]\n",
      "Returned loss:  9.858045527809544\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[4]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[4]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.862737655639648 and nTotal: 2\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367, 39.371742248535156, 19.725475311279297]\n",
      "Returned loss:  9.858492397126698\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[5]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[5]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.861873626708984 and nTotal: 2\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367, 39.371742248535156, 19.725475311279297, 19.72374725341797]\n",
      "Returned loss:  9.858786417090375\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[6]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[6]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.857037544250488 and nTotal: 1\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367, 39.371742248535156, 19.725475311279297, 19.72374725341797, 9.857037544250488]\n",
      "Returned loss:  9.858713547388712\n",
      "<--- DONE WITH ONE TIMESTEP --->\n",
      "Decoder output: torch.Size([5, 18784]) and Decoder Hidden: torch.Size([2, 5, 512])\n",
      "Target variable at timestep[7]: torch.Size([5]) and Decoder Input: torch.Size([1, 5])\n",
      "Mask at timestep[7]: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8) with shape torch.Size([8, 5])\n",
      "Loss: 9.854768753051758 and nTotal: 1\n",
      "print_losses: [49.295592308044434, 49.299612045288086, 49.33591842651367, 39.371742248535156, 19.725475311279297, 19.72374725341797, 9.857037544250488, 9.854768753051758]\n",
      "Returned loss:  9.858555755615235\n",
      "<--- DONE WITH ONE TIMESTEP --->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/r4mdm1_n5s5c4cvws96yn7qm0000gn/T/ipykernel_24647/1866906421.py:7: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /Users/runner/miniforge3/conda-bld/pytorch-recipe_1635217266490/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1356.)\n",
      "  loss = crossEntropy.masked_select(mask)\n"
     ]
    }
   ],
   "source": [
    "#Training demo for 1 time step\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "small_batch_size = 5\n",
    "\n",
    "batches = batch2TrainData(voc, [random.choice(trimmed_pairs) for _ in range(small_batch_size)])\n",
    "\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "\n",
    "#Parameters defining\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "atten_method = 'dot'\n",
    "embedding_maxtrix = nn.Embedding(voc.n_words, hidden_size)\n",
    "\n",
    "\n",
    "#Architecture defining\n",
    "encoder = GRUEncoder(hidden_size, embedding_maxtrix, encoder_n_layers, dropout)\n",
    "decoder = GRUDecoderAttn(atten_method, embedding_maxtrix, hidden_size, voc.n_words, decoder_n_layers, dropout)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "#Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "#Initializing optimizers\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "#clearing the gradient buffer on each iteration\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "input_variable.to(device)\n",
    "lengths.to(device)\n",
    "target_variable.to(device)\n",
    "mask.to(device)\n",
    "\n",
    "loss = 0\n",
    "print_losses = []\n",
    "n_totals = 0\n",
    "\n",
    "encoder_output, encoder_hidden = encoder(input_variable, lengths)\n",
    "# print(encode_output)\n",
    "print('Encoder output is of {} and Encoder hidden is of {} shapes'.format(\n",
    "    encoder_output.shape, encoder_hidden.shape))\n",
    "print('*'*100)\n",
    "\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(small_batch_size)]])\n",
    "decoder_input = decoder_input.to(device)\n",
    "print('Initial Decoder input is of {} shapes'.format(decoder_input.shape))\n",
    "# print('*'*100)\n",
    "\n",
    "#setting the initial decoder hidden state to encoder's final hidden state\n",
    "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "print('Initial Decoder hidden state is of {} shape'.format(decoder_hidden.shape))\n",
    "\n",
    "print('-'*100)\n",
    "print('<--Inside the one time step of GRU-->')\n",
    "\n",
    "\n",
    "#if you using teacher forcing\n",
    "for timestep in range(max_target_len):\n",
    "    \n",
    "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "    print('Decoder output: {} and Decoder Hidden: {}'.format(decoder_output.shape, decoder_hidden.shape))\n",
    "    \n",
    "    #In Teacher forcing: Next input is current target\n",
    "    decoder_input = target_variable[timestep].view(1,-1)\n",
    "    \n",
    "    print('Target variable at timestep[{}]: {} and Decoder Input: {}'.format(\n",
    "        timestep, target_variable[timestep].shape, decoder_input.shape))\n",
    "    \n",
    "    print('Mask at timestep[{}]: {} with shape {}'.format(timestep, mask, mask.shape))\n",
    "    maskLoss, nTotal = maskNLLLoss(decoder_output, target_variable[timestep], mask[timestep])\n",
    "    \n",
    "    print('Loss: {} and nTotal: {}'.format(maskLoss, nTotal))\n",
    "    \n",
    "    loss += maskLoss\n",
    "    \n",
    "    print_losses.append(maskLoss.item() * nTotal)\n",
    "    \n",
    "    print('print_losses: {}'.format(print_losses))\n",
    "    \n",
    "    n_totals += nTotal\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    returned_loss = sum(print_losses) / n_totals\n",
    "    \n",
    "    print('Returned loss: ', returned_loss)\n",
    "    \n",
    "    print('<--- DONE WITH ONE TIMESTEP --->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "852ed18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding_matrix,\n",
    "         encoder_optimizer, decoder_optimizer, batch_size, clip, max_len=MAX_LEN):\n",
    "    \n",
    "    #zero_gradient\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #set device option\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    #variable initialize\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "    \n",
    "    #forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "#     print(encoder_outputs.shape)\n",
    "    \n",
    "    #create initial decoder input (start the each sentence with SOS token)\n",
    "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    #setting the initial decoder hidden state to encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "#     print(decoder_hidden.shape)\n",
    "    \n",
    "    #determine if we're using Teacher Forcing for this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        for timestep in range(max_target_len):\n",
    "            \n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #In Teacher forcing: Next input is current target\n",
    "            decoder_input = target_variable[timestep].view(1,-1)\n",
    "            maskLoss, nTotal = maskNLLLoss(decoder_output, target_variable[timestep], mask[timestep])\n",
    "            loss += maskLoss\n",
    "            print_losses.append(maskLoss.item() * nTotal)\n",
    "    #         print('print_losses: {}'.format(print_losses))\n",
    "            n_totals += nTotal\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for timestep in range(max_target_len):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            #No Teacher forcing therefore, next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[item][0] for item in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            #calculate and accumlate loss\n",
    "            maskLoss, nTotal = maskNLLLoss(decoder_output, target_variable[timestep], mask[timestep])\n",
    "            loss += maskLoss\n",
    "            print_losses.append(maskLoss.item() * nTotal)\n",
    "    #         print('print_losses: {}'.format(print_losses))\n",
    "            n_totals += nTotal\n",
    "        \n",
    "    #perform backpropogation\n",
    "    loss.backward()\n",
    "    \n",
    "    #Clip gradient: Gradients are modified in-place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    #adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f536dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIter(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "             embedding_matrix, encoder_n_layers, decoder_n_layers, save_dir, n_iterations, batch_size,\n",
    "             print_every, save_every, clip, corpus_name, loadFileName):\n",
    "    \n",
    "    #loading batches for each iteration\n",
    "    training_batches = [batch2TrainData(vocab, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(n_iterations)]\n",
    "    \n",
    "    \n",
    "    print('Initializing....')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFileName:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "        \n",
    "    #Training loop\n",
    "    print('Training....')\n",
    "    for iteration in range(start_iteration, n_iterations - 1):\n",
    "        \n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder,\n",
    "                    embedding_matrix, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        \n",
    "        print_loss += loss\n",
    "        \n",
    "        \n",
    "        #print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print('Iteration: {}, Avg loss: {:4f}'.format(iteration, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            \n",
    "            \n",
    "        #save checkpoint\n",
    "        if (iteration % save_every) == 0:\n",
    "            \n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(\n",
    "                encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "        \n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'encoder': encoder.state_dict(),\n",
    "                'decoder': decoder.state_dict(),\n",
    "                'en_optimizer': encoder_optimizer.state_dict(),\n",
    "                'de_optimizer': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding_matrix.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c9623da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/chatbot_model/cornell_movies/2-2_512/3500_checkpoint.tar\n",
      "Building encoder and decoder ...\n",
      "GRUEncoder(\n",
      "  (embedding): Embedding(18784, 512)\n",
      "  (gru): GRU(512, 512, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ") GRUDecoderAttn(\n",
      "  (embedding): Embedding(18784, 512)\n",
      "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gru): GRU(512, 512, num_layers=2, dropout=0.1)\n",
      "  (concat): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=18784, bias=True)\n",
      "  (atten): Attention()\n",
      ")\n",
      "Models built and ready to go, In eval mode!\n"
     ]
    }
   ],
   "source": [
    "#Initializing Encoder-Decoder\n",
    "\n",
    "# Configure models\n",
    "model_name = 'chatbot_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "save_dir = '../models/'\n",
    "corpus_name = 'cornell_movies'\n",
    "\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 3500\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "print(loadFilename)\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['encoder']\n",
    "    decoder_sd = checkpoint['decoder']\n",
    "    encoder_optimizer_sd = checkpoint['en_optimizer']\n",
    "    decoder_optimizer_sd = checkpoint['de_optimizer']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.n_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = GRUEncoder(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = GRUDecoderAttn(attn_model, embedding, hidden_size, voc.n_words, decoder_n_layers, dropout)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "    \n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "print(encoder, decoder)\n",
    "if loadFilename:\n",
    "    print('Models built and ready to go, In eval mode!')\n",
    "    \n",
    "else: print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f624e4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "save_dir = '../models/'\n",
    "corpus_name = 'cornell_movies'\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "    \n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIter(model_name, voc, trimmed_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796cca2",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3950c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_TOKEN\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "450eb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LEN):\n",
    "    \n",
    "    indexes_batch = [sentence_indexing(voc, sentence)]\n",
    "    \n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "\n",
    "    decoded_words = [voc.idx2word[token.item()] for token in tokens]\n",
    "    \n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evalInput(encoder, decoder, searcher, voc):\n",
    "    \n",
    "    input_sentence = ''\n",
    "    \n",
    "    while(1):\n",
    "        \n",
    "        try:\n",
    "            input_sequence = input('> ')\n",
    "            if input_sequence == 'q' or input_sequence == 'quit':\n",
    "                break\n",
    "            input_sequence = normalizeString(input_sequence)\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sequence)\n",
    "            output_words[:] = [item for item in output_words if not (item == 'PAD' or item == 'EOS')]\n",
    "            print('Bot: ', ' '.join(output_words))\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fd30430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "# evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f429c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hey\n",
      "Bot:  You know what I mean.\n",
      "> how are you\n",
      "Bot:  I'm not sure.\n",
      "> why\n",
      "Bot:  Because I'm a little while, of them.\n",
      "> cool. when would you come here?\n",
      "Bot:  I don't know.\n",
      "> okay. How is weather at your end?\n",
      "Bot:  My wife.\n",
      "> lol\n",
      "Error: Encountered unknown word.\n",
      "> q\n"
     ]
    }
   ],
   "source": [
    "evalInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2bb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
