{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b74e0a",
   "metadata": {},
   "source": [
    "# CBOW (Continuous Bag-of-Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d33dbd",
   "metadata": {},
   "source": [
    "Part of **#30DaysOfBasics**, Imma implement CBOW using pytorh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23951301",
   "metadata": {},
   "source": [
    "CBOW: Given conext words and it would predicts the word. This is distinct from LM, since it is not sequential model and does not have to be probabilistic. Typically, CBOW is used to quickly train word embeddings, and these embeddings are used to initialize the embeddings of some more complicated model. Usually, this is referred to as pretraining embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d3acf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1e7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_FILE_PATH = './data/data/hi/hi_sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b86f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "\n",
    "EMBD_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7087f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To re-use method of N-gram notebook\n",
    "\n",
    "%run 'n-gram LM.ipynb'\n",
    "hi_vocab, hi_word_to_idx, hi_idx_to_word, hi_content = create_vocab(HI_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dcc80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['अस्पताल', 'में', 'डिलीवरी', 'के', 'लिए', 'लेबर', 'रूम', 'बना', 'है,', 'लेकिन']\n"
     ]
    }
   ],
   "source": [
    "print(hi_content[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9049d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_word_to_ix = {word:ix for ix, word in enumerate(hi_vocab)}\n",
    "hi_ix_to_word = {ix:word for ix, word in enumerate(hi_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe34b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_data(list_of_tokens, window=2):\n",
    "    cbow = [(\n",
    "    [list_of_tokens[idx-idx1-1] for idx1 in range(window)] +\n",
    "    [list_of_tokens[idx+idx1+1] for idx1 in range(window)],\n",
    "\n",
    "    list_of_tokens[idx])\n",
    " \n",
    "    for idx in range(window, len(list_of_tokens)-window\n",
    "    )\n",
    "    ]\n",
    "    return cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa87958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_data = generate_cbow_data(hi_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "517b42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        print(embeds.shape)\n",
    "        out = F.relu(self.fc1(embeds))\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0f47e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CBOW(len(hi_vocab), EMBD_DIM, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc078c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for context, target in cbow_data:\n",
    "        \n",
    "        context_tensor = torch.tensor([hi_word_to_ix[word] for word in context], dtype=torch.long)\n",
    "        \n",
    "        network.zero_grad()\n",
    "        \n",
    "        words_probs = network(context_tensor)\n",
    "        \n",
    "        loss = loss_fn(words_prob, torch.tensor([hi_word_to_ix[target]], dtype=torch.long))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    total_loss += loss.item()\n",
    "    print('Epoch {}/{}, loss: {}'.format(epoch, EPOCHS, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f86938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
