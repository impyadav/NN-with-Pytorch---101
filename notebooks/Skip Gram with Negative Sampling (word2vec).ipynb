{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd538b2",
   "metadata": {},
   "source": [
    "# Skip gram with Negative sampling (SGNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313f038",
   "metadata": {},
   "source": [
    "Part of **#30DaysOfBasics!**\n",
    "\n",
    "\n",
    "Contrary to CBOW, Skip-gram takes the word and predict its context within specified window. It is very computation expensive (when vocab size is very big), later, in 2013, Mikolov and the team of researchers provived several extensions of skip gram to improve both the quality of the vectors and the training\n",
    "speed under the paper titled **'Distributed Representations of Words and Phrases\n",
    "and their Compositionality** and 'Skip gram with negative sampling' was one of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18543458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c620e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HI_FILE_PATH = '/Users/impyadav/Desktop/data/data/hi/hi_sample.txt'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "CONTEXT_WINDOW = 3\n",
    "EMBED_DIM=128\n",
    "EPOCHS=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1109ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "#Prepare the vocabulary\n",
    "\n",
    "def create_vocab(text_file):\n",
    "    with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    vocab = set(content.split())\n",
    "    word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "    ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "    return vocab, word_to_ix, ix_to_word, content.split()\n",
    "\n",
    "\n",
    "hi_vocab, hi_word_to_ix, hi_ix_to_word, hi_content = create_vocab(HI_FILE_PATH)\n",
    "\n",
    "print(len(hi_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13051dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the negative samples for given idx\n",
    "\n",
    "def generate_neg_samples(idx, list_of_tokens, context_window, k):\n",
    "    \n",
    "    pos_index = range(idx-context_window, idx+context_window+1)\n",
    "    updated_idxs = set(range(len(list_of_tokens))).difference(set(pos_index))\n",
    "    \n",
    "    return random.sample(updated_idxs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2fa1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate samples\n",
    "\n",
    "def generate_sgns_data(list_of_tokens, context_window, k):\n",
    "    \n",
    "    sg_data = []\n",
    "    \n",
    "    for idx in range(context_window, len(list_of_tokens)-context_window):\n",
    "        \n",
    "        temp = []\n",
    "        pre_context = [list_of_tokens[idx-idx1-1] for idx1 in range(context_window)]\n",
    "        post_context = [list_of_tokens[idx+idx1+1] for idx1 in range(context_window)]\n",
    "        \n",
    "        for context in pre_context + post_context:\n",
    "            temp.append([list_of_tokens[idx], context, 1])\n",
    "            \n",
    "        temp += [[list_of_tokens[idx], list_of_tokens[idx1], 0] for idx1 in generate_neg_samples(idx, list_of_tokens, context_window, k)]\n",
    "        \n",
    "        sg_data.append(temp)\n",
    "        \n",
    "    return sg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ee6858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/r4mdm1_n5s5c4cvws96yn7qm0000gn/T/ipykernel_32877/1605369282.py:8: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  return random.sample(updated_idxs, k)\n"
     ]
    }
   ],
   "source": [
    "sgns_data = generate_sgns_data(hi_content, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d018662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['लिए', 'के', 1], ['लिए', 'डिलीवरी', 1], ['लिए', 'में', 1], ['लिए', 'अस्पताल', 1], ['लिए', 'लेबर', 1], ['लिए', 'रूम', 1], ['लिए', 'बना', 1], ['लिए', 'है,', 1], ['लिए', 'कॉपी', 0], ['लिए', 'उत्पन्न', 0], ['लिए', 'बेहतर', 0], ['लिए', 'फीसदी', 0], ['लिए', 'बी.', 0], ['लिए', 'कहा-', 0], ['लिए', 'के', 0], ['लिए', 'फिल्म', 0]], [['लेबर', 'लिए', 1], ['लेबर', 'के', 1], ['लेबर', 'डिलीवरी', 1], ['लेबर', 'में', 1], ['लेबर', 'रूम', 1], ['लेबर', 'बना', 1], ['लेबर', 'है,', 1], ['लेबर', 'लेकिन', 1], ['लेबर', '15:1-2,', 0], ['लेबर', 'करता', 0], ['लेबर', 'इस', 0], ['लेबर', 'ही', 0], ['लेबर', 'रीति', 0], ['लेबर', 'को', 0], ['लेबर', 'यह', 0], ['लेबर', 'हजार', 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(sgns_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4a90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the network\n",
    "\n",
    "class SGNS(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SGNS, self).__init__()\n",
    "        self.input_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_word, context):\n",
    "        \n",
    "        input_embedding = self.input_embedding(input_word).view(1,-1)\n",
    "        context_embedding = self.context_embedding(context)\n",
    "        \n",
    "        context_embedding = torch.transpose(context_embedding,0,1)\n",
    "        \n",
    "        dot = torch.mm(input_embedding, context_embedding)\n",
    "        \n",
    "        scores = F.softmax(dot)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def2e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = SGNS(len(hi_vocab), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657e74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0c4486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 and loss: 3665.3501102924347\n",
      "Epoch 1/500 and loss: 3719.2619240283966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fm/r4mdm1_n5s5c4cvws96yn7qm0000gn/T/ipykernel_32877/4133125937.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  scores = F.softmax(dot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500 and loss: 3702.36297249794\n",
      "Epoch 3/500 and loss: 3685.57830786705\n",
      "Epoch 4/500 and loss: 3668.915704727173\n",
      "Epoch 5/500 and loss: 3652.387958049774\n",
      "Epoch 6/500 and loss: 3635.9967012405396\n",
      "Epoch 7/500 and loss: 3619.703288078308\n",
      "Epoch 8/500 and loss: 3603.5487689971924\n",
      "Epoch 9/500 and loss: 3587.6073784828186\n",
      "Epoch 10/500 and loss: 3571.8163356781006\n",
      "Epoch 11/500 and loss: 3556.1648766994476\n",
      "Epoch 12/500 and loss: 3540.7042322158813\n",
      "Epoch 13/500 and loss: 3525.4333777427673\n",
      "Epoch 14/500 and loss: 3510.3021664619446\n",
      "Epoch 15/500 and loss: 3495.3516652584076\n",
      "Epoch 16/500 and loss: 3480.5602235794067\n",
      "Epoch 17/500 and loss: 3465.9268715381622\n",
      "Epoch 18/500 and loss: 3451.465877056122\n",
      "Epoch 19/500 and loss: 3437.1852309703827\n",
      "Epoch 20/500 and loss: 3423.0838873386383\n",
      "Epoch 21/500 and loss: 3409.1708998680115\n",
      "Epoch 22/500 and loss: 3395.454136610031\n",
      "Epoch 23/500 and loss: 3381.9435057640076\n",
      "Epoch 24/500 and loss: 3368.648289203644\n",
      "Epoch 25/500 and loss: 3355.564219236374\n",
      "Epoch 26/500 and loss: 3342.684793949127\n",
      "Epoch 27/500 and loss: 3329.998124361038\n",
      "Epoch 28/500 and loss: 3317.496984243393\n",
      "Epoch 29/500 and loss: 3305.160279750824\n",
      "Epoch 30/500 and loss: 3292.982438325882\n",
      "Epoch 31/500 and loss: 3280.950203180313\n",
      "Epoch 32/500 and loss: 3269.05028796196\n",
      "Epoch 33/500 and loss: 3257.275858640671\n",
      "Epoch 34/500 and loss: 3245.6260375976562\n",
      "Epoch 35/500 and loss: 3234.0944304466248\n",
      "Epoch 36/500 and loss: 3222.6789705753326\n",
      "Epoch 37/500 and loss: 3211.3728573322296\n",
      "Epoch 38/500 and loss: 3200.1755595207214\n",
      "Epoch 39/500 and loss: 3189.0878891944885\n",
      "Epoch 40/500 and loss: 3178.116630554199\n",
      "Epoch 41/500 and loss: 3167.271964788437\n",
      "Epoch 42/500 and loss: 3156.5794219970703\n",
      "Epoch 43/500 and loss: 3146.158082008362\n",
      "Epoch 44/500 and loss: 3137.9634613990784\n",
      "Epoch 45/500 and loss: 3121.700432777405\n",
      "Epoch 46/500 and loss: 3111.198772907257\n",
      "Epoch 47/500 and loss: 3100.7917037010193\n",
      "Epoch 48/500 and loss: 3090.4779365062714\n",
      "Epoch 49/500 and loss: 3080.2542333602905\n",
      "Epoch 50/500 and loss: 3070.1189036369324\n",
      "Epoch 51/500 and loss: 3060.070473432541\n",
      "Epoch 52/500 and loss: 3050.107304573059\n",
      "Epoch 53/500 and loss: 3040.228208065033\n",
      "Epoch 54/500 and loss: 3030.4308111667633\n",
      "Epoch 55/500 and loss: 3020.712447166443\n",
      "Epoch 56/500 and loss: 3011.071215391159\n",
      "Epoch 57/500 and loss: 3001.506224632263\n",
      "Epoch 58/500 and loss: 2992.0152184963226\n",
      "Epoch 59/500 and loss: 2982.59933757782\n",
      "Epoch 60/500 and loss: 2973.2606112957\n",
      "Epoch 61/500 and loss: 2963.9990241527557\n",
      "Epoch 62/500 and loss: 2954.81237077713\n",
      "Epoch 63/500 and loss: 2945.6998360157013\n",
      "Epoch 64/500 and loss: 2936.6599147319794\n",
      "Epoch 65/500 and loss: 2927.691185235977\n",
      "Epoch 66/500 and loss: 2918.791494846344\n",
      "Epoch 67/500 and loss: 2909.961140871048\n",
      "Epoch 68/500 and loss: 2901.199471950531\n",
      "Epoch 69/500 and loss: 2892.5049319267273\n",
      "Epoch 70/500 and loss: 2883.8756964206696\n",
      "Epoch 71/500 and loss: 2875.3107693195343\n",
      "Epoch 72/500 and loss: 2866.808310031891\n",
      "Epoch 73/500 and loss: 2858.3662049770355\n",
      "Epoch 74/500 and loss: 2849.9839680194855\n",
      "Epoch 75/500 and loss: 2841.6613852977753\n",
      "Epoch 76/500 and loss: 2833.398485660553\n",
      "Epoch 77/500 and loss: 2825.1960532665253\n",
      "Epoch 78/500 and loss: 2817.0551493167877\n",
      "Epoch 79/500 and loss: 2808.9761970043182\n",
      "Epoch 80/500 and loss: 2800.957947731018\n",
      "Epoch 81/500 and loss: 2792.9991402626038\n",
      "Epoch 82/500 and loss: 2785.0992732048035\n",
      "Epoch 83/500 and loss: 2777.259207010269\n",
      "Epoch 84/500 and loss: 2769.47785282135\n",
      "Epoch 85/500 and loss: 2761.7541296482086\n",
      "Epoch 86/500 and loss: 2754.088799238205\n",
      "Epoch 87/500 and loss: 2746.4810440540314\n",
      "Epoch 88/500 and loss: 2738.9304111003876\n",
      "Epoch 89/500 and loss: 2731.4360797405243\n",
      "Epoch 90/500 and loss: 2723.9970421791077\n",
      "Epoch 91/500 and loss: 2716.6116268634796\n",
      "Epoch 92/500 and loss: 2709.2783682346344\n",
      "Epoch 93/500 and loss: 2701.996077299118\n",
      "Epoch 94/500 and loss: 2694.764118909836\n",
      "Epoch 95/500 and loss: 2687.581209421158\n",
      "Epoch 96/500 and loss: 2680.445461511612\n",
      "Epoch 97/500 and loss: 2673.3560359477997\n",
      "Epoch 98/500 and loss: 2666.3123486042023\n",
      "Epoch 99/500 and loss: 2659.3122515678406\n",
      "Epoch 100/500 and loss: 2652.354566335678\n",
      "Epoch 101/500 and loss: 2645.4378337860107\n",
      "Epoch 102/500 and loss: 2638.5604412555695\n",
      "Epoch 103/500 and loss: 2631.720808982849\n",
      "Epoch 104/500 and loss: 2624.9178421497345\n",
      "Epoch 105/500 and loss: 2618.1490288972855\n",
      "Epoch 106/500 and loss: 2611.414557337761\n",
      "Epoch 107/500 and loss: 2604.714020729065\n",
      "Epoch 108/500 and loss: 2598.046790957451\n",
      "Epoch 109/500 and loss: 2591.4132964611053\n",
      "Epoch 110/500 and loss: 2584.8128675222397\n",
      "Epoch 111/500 and loss: 2578.2455345392227\n",
      "Epoch 112/500 and loss: 2571.709561109543\n",
      "Epoch 113/500 and loss: 2565.2036683559418\n",
      "Epoch 114/500 and loss: 2558.729117155075\n",
      "Epoch 115/500 and loss: 2552.285325527191\n",
      "Epoch 116/500 and loss: 2545.87182867527\n",
      "Epoch 117/500 and loss: 2539.492165207863\n",
      "Epoch 118/500 and loss: 2533.146450281143\n",
      "Epoch 119/500 and loss: 2526.8337728977203\n",
      "Epoch 120/500 and loss: 2520.5530601739883\n",
      "Epoch 121/500 and loss: 2514.3036448955536\n",
      "Epoch 122/500 and loss: 2508.0858676433563\n",
      "Epoch 123/500 and loss: 2501.89894926548\n",
      "Epoch 124/500 and loss: 2495.7427887916565\n",
      "Epoch 125/500 and loss: 2489.6164700984955\n",
      "Epoch 126/500 and loss: 2483.519923567772\n",
      "Epoch 127/500 and loss: 2477.4533174037933\n",
      "Epoch 128/500 and loss: 2471.416732907295\n",
      "Epoch 129/500 and loss: 2465.410300850868\n",
      "Epoch 130/500 and loss: 2459.433467745781\n",
      "Epoch 131/500 and loss: 2453.4854443073273\n",
      "Epoch 132/500 and loss: 2447.5667498111725\n",
      "Epoch 133/500 and loss: 2441.677325963974\n",
      "Epoch 134/500 and loss: 2435.816997051239\n",
      "Epoch 135/500 and loss: 2429.9860755205154\n",
      "Epoch 136/500 and loss: 2424.183455824852\n",
      "Epoch 137/500 and loss: 2418.408733725548\n",
      "Epoch 138/500 and loss: 2412.661912083626\n",
      "Epoch 139/500 and loss: 2406.9426894187927\n",
      "Epoch 140/500 and loss: 2401.2539409399033\n",
      "Epoch 141/500 and loss: 2395.5956897735596\n",
      "Epoch 142/500 and loss: 2389.967210292816\n",
      "Epoch 143/500 and loss: 2384.3679485321045\n",
      "Epoch 144/500 and loss: 2378.7976285219193\n",
      "Epoch 145/500 and loss: 2373.2552486658096\n",
      "Epoch 146/500 and loss: 2367.7414078712463\n",
      "Epoch 147/500 and loss: 2362.255216240883\n",
      "Epoch 148/500 and loss: 2356.796402335167\n",
      "Epoch 149/500 and loss: 2351.364234805107\n",
      "Epoch 150/500 and loss: 2345.958102107048\n",
      "Epoch 151/500 and loss: 2340.5774354934692\n",
      "Epoch 152/500 and loss: 2335.2210173606873\n",
      "Epoch 153/500 and loss: 2329.8886420726776\n",
      "Epoch 154/500 and loss: 2324.580092191696\n",
      "Epoch 155/500 and loss: 2319.294858932495\n",
      "Epoch 156/500 and loss: 2314.0329126119614\n",
      "Epoch 157/500 and loss: 2308.793565392494\n",
      "Epoch 158/500 and loss: 2303.5768040418625\n",
      "Epoch 159/500 and loss: 2298.38327562809\n",
      "Epoch 160/500 and loss: 2293.2132630348206\n",
      "Epoch 161/500 and loss: 2288.0667510032654\n",
      "Epoch 162/500 and loss: 2282.9445036649704\n",
      "Epoch 163/500 and loss: 2277.846182703972\n",
      "Epoch 164/500 and loss: 2272.771794319153\n",
      "Epoch 165/500 and loss: 2267.7203084230423\n",
      "Epoch 166/500 and loss: 2262.6917304992676\n",
      "Epoch 167/500 and loss: 2257.6853646039963\n",
      "Epoch 168/500 and loss: 2252.701290369034\n",
      "Epoch 169/500 and loss: 2247.738580107689\n",
      "Epoch 170/500 and loss: 2242.7977434396744\n",
      "Epoch 171/500 and loss: 2237.879697918892\n",
      "Epoch 172/500 and loss: 2232.9836364984512\n",
      "Epoch 173/500 and loss: 2228.1090039014816\n",
      "Epoch 174/500 and loss: 2223.2554762363434\n",
      "Epoch 175/500 and loss: 2218.422045946121\n",
      "Epoch 176/500 and loss: 2213.6084554195404\n",
      "Epoch 177/500 and loss: 2208.813818693161\n",
      "Epoch 178/500 and loss: 2204.037294626236\n",
      "Epoch 179/500 and loss: 2199.2778660058975\n",
      "Epoch 180/500 and loss: 2194.5344500541687\n",
      "Epoch 181/500 and loss: 2189.805673122406\n",
      "Epoch 182/500 and loss: 2185.090386748314\n",
      "Epoch 183/500 and loss: 2180.386783719063\n",
      "Epoch 184/500 and loss: 2175.6974292993546\n",
      "Epoch 185/500 and loss: 2171.025130748749\n",
      "Epoch 186/500 and loss: 2166.369629263878\n",
      "Epoch 187/500 and loss: 2161.72982108593\n",
      "Epoch 188/500 and loss: 2157.1040115356445\n",
      "Epoch 189/500 and loss: 2152.4906980991364\n",
      "Epoch 190/500 and loss: 2147.8885148763657\n",
      "Epoch 191/500 and loss: 2143.2967941761017\n",
      "Epoch 192/500 and loss: 2138.7148472070694\n",
      "Epoch 193/500 and loss: 2134.140944480896\n",
      "Epoch 194/500 and loss: 2129.5792224407196\n",
      "Epoch 195/500 and loss: 2125.031598687172\n",
      "Epoch 196/500 and loss: 2120.4965422153473\n",
      "Epoch 197/500 and loss: 2115.9726637601852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500 and loss: 2111.4603332281113\n",
      "Epoch 199/500 and loss: 2106.96286380291\n",
      "Epoch 200/500 and loss: 2102.483209133148\n",
      "Epoch 201/500 and loss: 2098.020444869995\n",
      "Epoch 202/500 and loss: 2093.5737779140472\n",
      "Epoch 203/500 and loss: 2089.1435916423798\n",
      "Epoch 204/500 and loss: 2084.7299571037292\n",
      "Epoch 205/500 and loss: 2080.332409143448\n",
      "Epoch 206/500 and loss: 2075.9509679079056\n",
      "Epoch 207/500 and loss: 2071.5856779813766\n",
      "Epoch 208/500 and loss: 2067.23602104187\n",
      "Epoch 209/500 and loss: 2062.901240706444\n",
      "Epoch 210/500 and loss: 2058.5803487300873\n",
      "Epoch 211/500 and loss: 2054.273577809334\n",
      "Epoch 212/500 and loss: 2049.980792284012\n",
      "Epoch 213/500 and loss: 2045.702199101448\n",
      "Epoch 214/500 and loss: 2041.4368464946747\n",
      "Epoch 215/500 and loss: 2037.183542728424\n",
      "Epoch 216/500 and loss: 2032.9411259889603\n",
      "Epoch 217/500 and loss: 2028.7092700004578\n",
      "Epoch 218/500 and loss: 2024.4863681793213\n",
      "Epoch 219/500 and loss: 2020.2711700201035\n",
      "Epoch 220/500 and loss: 2016.0675723552704\n",
      "Epoch 221/500 and loss: 2011.8778676986694\n",
      "Epoch 222/500 and loss: 2007.7007567882538\n",
      "Epoch 223/500 and loss: 2003.5355365276337\n",
      "Epoch 224/500 and loss: 1999.383118391037\n",
      "Epoch 225/500 and loss: 1995.2439712285995\n",
      "Epoch 226/500 and loss: 1991.1172549724579\n",
      "Epoch 227/500 and loss: 1987.002417087555\n",
      "Epoch 228/500 and loss: 1982.8986942768097\n",
      "Epoch 229/500 and loss: 1978.8050581216812\n",
      "Epoch 230/500 and loss: 1974.7200766801834\n",
      "Epoch 231/500 and loss: 1970.643255829811\n",
      "Epoch 232/500 and loss: 1966.5751477479935\n",
      "Epoch 233/500 and loss: 1962.5137774944305\n",
      "Epoch 234/500 and loss: 1958.4642736911774\n",
      "Epoch 235/500 and loss: 1954.4287869930267\n",
      "Epoch 236/500 and loss: 1950.4066734313965\n",
      "Epoch 237/500 and loss: 1946.3973914384842\n",
      "Epoch 238/500 and loss: 1942.4003763198853\n",
      "Epoch 239/500 and loss: 1938.4151450395584\n",
      "Epoch 240/500 and loss: 1934.4408731460571\n",
      "Epoch 241/500 and loss: 1930.4772511720657\n",
      "Epoch 242/500 and loss: 1926.5234652757645\n",
      "Epoch 243/500 and loss: 1922.57772898674\n",
      "Epoch 244/500 and loss: 1918.6378294229507\n",
      "Epoch 245/500 and loss: 1914.7040088176727\n",
      "Epoch 246/500 and loss: 1910.7813700437546\n",
      "Epoch 247/500 and loss: 1906.8703167438507\n",
      "Epoch 248/500 and loss: 1902.9697870016098\n",
      "Epoch 249/500 and loss: 1899.0789732933044\n",
      "Epoch 250/500 and loss: 1895.1972378492355\n",
      "Epoch 251/500 and loss: 1891.323550105095\n",
      "Epoch 252/500 and loss: 1887.455834031105\n",
      "Epoch 253/500 and loss: 1883.5913568735123\n",
      "Epoch 254/500 and loss: 1879.7261568307877\n",
      "Epoch 255/500 and loss: 1875.8543446063995\n",
      "Epoch 256/500 and loss: 1871.9664158821106\n",
      "Epoch 257/500 and loss: 1868.0632269382477\n",
      "Epoch 258/500 and loss: 1864.1719130277634\n",
      "Epoch 259/500 and loss: 1860.2987196445465\n",
      "Epoch 260/500 and loss: 1856.444412112236\n",
      "Epoch 261/500 and loss: 1852.610237956047\n",
      "Epoch 262/500 and loss: 1848.7975301742554\n",
      "Epoch 263/500 and loss: 1845.0059597492218\n",
      "Epoch 264/500 and loss: 1841.2346699237823\n",
      "Epoch 265/500 and loss: 1837.4821190834045\n",
      "Epoch 266/500 and loss: 1833.7464287281036\n",
      "Epoch 267/500 and loss: 1830.0275710821152\n",
      "Epoch 268/500 and loss: 1826.3267267942429\n",
      "Epoch 269/500 and loss: 1822.6412568092346\n",
      "Epoch 270/500 and loss: 1818.9685773849487\n",
      "Epoch 271/500 and loss: 1815.3074210882187\n",
      "Epoch 272/500 and loss: 1811.6596522331238\n",
      "Epoch 273/500 and loss: 1808.0273578166962\n",
      "Epoch 274/500 and loss: 1804.40993142128\n",
      "Epoch 275/500 and loss: 1800.8069447278976\n",
      "Epoch 276/500 and loss: 1797.218111038208\n",
      "Epoch 277/500 and loss: 1793.6434489488602\n",
      "Epoch 278/500 and loss: 1790.083508849144\n",
      "Epoch 279/500 and loss: 1786.5383785963058\n",
      "Epoch 280/500 and loss: 1783.0084735155106\n",
      "Epoch 281/500 and loss: 1779.4941127300262\n",
      "Epoch 282/500 and loss: 1775.99587225914\n",
      "Epoch 283/500 and loss: 1772.5140746831894\n",
      "Epoch 284/500 and loss: 1769.0502027273178\n",
      "Epoch 285/500 and loss: 1765.60466837883\n",
      "Epoch 286/500 and loss: 1762.1775691509247\n",
      "Epoch 287/500 and loss: 1758.7690752744675\n",
      "Epoch 288/500 and loss: 1755.379858970642\n",
      "Epoch 289/500 and loss: 1752.0098522901535\n",
      "Epoch 290/500 and loss: 1748.658907532692\n",
      "Epoch 291/500 and loss: 1745.3266389369965\n",
      "Epoch 292/500 and loss: 1742.0127555131912\n",
      "Epoch 293/500 and loss: 1738.7168345451355\n",
      "Epoch 294/500 and loss: 1735.4384838342667\n",
      "Epoch 295/500 and loss: 1732.1773298978806\n",
      "Epoch 296/500 and loss: 1728.932679772377\n",
      "Epoch 297/500 and loss: 1725.7040966749191\n",
      "Epoch 298/500 and loss: 1722.4910558462143\n",
      "Epoch 299/500 and loss: 1719.292727470398\n",
      "Epoch 300/500 and loss: 1716.108099102974\n",
      "Epoch 301/500 and loss: 1712.936147928238\n",
      "Epoch 302/500 and loss: 1709.7766457796097\n",
      "Epoch 303/500 and loss: 1706.6320925951004\n",
      "Epoch 304/500 and loss: 1703.5032230615616\n",
      "Epoch 305/500 and loss: 1700.390886425972\n",
      "Epoch 306/500 and loss: 1697.294930934906\n",
      "Epoch 307/500 and loss: 1694.2151458263397\n",
      "Epoch 308/500 and loss: 1691.1511915922165\n",
      "Epoch 309/500 and loss: 1688.102690577507\n",
      "Epoch 310/500 and loss: 1685.069303035736\n",
      "Epoch 311/500 and loss: 1682.05060338974\n",
      "Epoch 312/500 and loss: 1679.0467907190323\n",
      "Epoch 313/500 and loss: 1676.0573415756226\n",
      "Epoch 314/500 and loss: 1673.0815774202347\n",
      "Epoch 315/500 and loss: 1670.119950413704\n",
      "Epoch 316/500 and loss: 1667.173320055008\n",
      "Epoch 317/500 and loss: 1664.2416611909866\n",
      "Epoch 318/500 and loss: 1661.3243106603622\n",
      "Epoch 319/500 and loss: 1658.4206275939941\n",
      "Epoch 320/500 and loss: 1655.5300110578537\n",
      "Epoch 321/500 and loss: 1652.651911020279\n",
      "Epoch 322/500 and loss: 1649.7858555316925\n",
      "Epoch 323/500 and loss: 1646.9310734272003\n",
      "Epoch 324/500 and loss: 1644.0879472494125\n",
      "Epoch 325/500 and loss: 1641.2563738822937\n",
      "Epoch 326/500 and loss: 1638.43621301651\n",
      "Epoch 327/500 and loss: 1635.627447128296\n",
      "Epoch 328/500 and loss: 1632.8298462629318\n",
      "Epoch 329/500 and loss: 1630.0430235862732\n",
      "Epoch 330/500 and loss: 1627.266787648201\n",
      "Epoch 331/500 and loss: 1624.5006932020187\n",
      "Epoch 332/500 and loss: 1621.7445908784866\n",
      "Epoch 333/500 and loss: 1618.9982707500458\n",
      "Epoch 334/500 and loss: 1616.2615612745285\n",
      "Epoch 335/500 and loss: 1613.534187078476\n",
      "Epoch 336/500 and loss: 1610.8161408901215\n",
      "Epoch 337/500 and loss: 1608.1071599721909\n",
      "Epoch 338/500 and loss: 1605.407286643982\n",
      "Epoch 339/500 and loss: 1602.7163546085358\n",
      "Epoch 340/500 and loss: 1600.0344495773315\n",
      "Epoch 341/500 and loss: 1597.361512541771\n",
      "Epoch 342/500 and loss: 1594.6974115371704\n",
      "Epoch 343/500 and loss: 1592.0419462919235\n",
      "Epoch 344/500 and loss: 1589.3951607942581\n",
      "Epoch 345/500 and loss: 1586.7568091154099\n",
      "Epoch 346/500 and loss: 1584.1268332004547\n",
      "Epoch 347/500 and loss: 1581.5050349235535\n",
      "Epoch 348/500 and loss: 1578.8912824392319\n",
      "Epoch 349/500 and loss: 1576.2853736877441\n",
      "Epoch 350/500 and loss: 1573.6870839595795\n",
      "Epoch 351/500 and loss: 1571.0962183475494\n",
      "Epoch 352/500 and loss: 1568.5125470161438\n",
      "Epoch 353/500 and loss: 1565.9362086057663\n",
      "Epoch 354/500 and loss: 1563.3672280311584\n",
      "Epoch 355/500 and loss: 1560.8054149150848\n",
      "Epoch 356/500 and loss: 1558.250565648079\n",
      "Epoch 357/500 and loss: 1555.7024245262146\n",
      "Epoch 358/500 and loss: 1553.1608016490936\n",
      "Epoch 359/500 and loss: 1550.6254560947418\n",
      "Epoch 360/500 and loss: 1548.0964930057526\n",
      "Epoch 361/500 and loss: 1545.5738208293915\n",
      "Epoch 362/500 and loss: 1543.057161450386\n",
      "Epoch 363/500 and loss: 1540.5461419820786\n",
      "Epoch 364/500 and loss: 1538.0403604507446\n",
      "Epoch 365/500 and loss: 1535.5393015146255\n",
      "Epoch 366/500 and loss: 1533.042951464653\n",
      "Epoch 367/500 and loss: 1530.5539373159409\n",
      "Epoch 368/500 and loss: 1528.0736882686615\n",
      "Epoch 369/500 and loss: 1525.6026301383972\n",
      "Epoch 370/500 and loss: 1523.1409608125687\n",
      "Epoch 371/500 and loss: 1520.6887947320938\n",
      "Epoch 372/500 and loss: 1518.2462177276611\n",
      "Epoch 373/500 and loss: 1515.8132046461105\n",
      "Epoch 374/500 and loss: 1513.389799118042\n",
      "Epoch 375/500 and loss: 1510.975984454155\n",
      "Epoch 376/500 and loss: 1508.5717210769653\n",
      "Epoch 377/500 and loss: 1506.177105665207\n",
      "Epoch 378/500 and loss: 1503.7920054197311\n",
      "Epoch 379/500 and loss: 1501.4161664247513\n",
      "Epoch 380/500 and loss: 1499.0494656562805\n",
      "Epoch 381/500 and loss: 1496.691574215889\n",
      "Epoch 382/500 and loss: 1494.3422315120697\n",
      "Epoch 383/500 and loss: 1492.0010432004929\n",
      "Epoch 384/500 and loss: 1489.6686208248138\n",
      "Epoch 385/500 and loss: 1487.3453607559204\n",
      "Epoch 386/500 and loss: 1485.0310053825378\n",
      "Epoch 387/500 and loss: 1482.725771188736\n",
      "Epoch 388/500 and loss: 1480.4294012784958\n",
      "Epoch 389/500 and loss: 1478.1417107582092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/500 and loss: 1475.8626005649567\n",
      "Epoch 391/500 and loss: 1473.5918862819672\n",
      "Epoch 392/500 and loss: 1471.3295992612839\n",
      "Epoch 393/500 and loss: 1469.0755769014359\n",
      "Epoch 394/500 and loss: 1466.8297072649002\n",
      "Epoch 395/500 and loss: 1464.5919170379639\n",
      "Epoch 396/500 and loss: 1462.3620351552963\n",
      "Epoch 397/500 and loss: 1460.1400171518326\n",
      "Epoch 398/500 and loss: 1457.9257514476776\n",
      "Epoch 399/500 and loss: 1455.7191257476807\n",
      "Epoch 400/500 and loss: 1453.520028591156\n",
      "Epoch 401/500 and loss: 1451.3283721208572\n",
      "Epoch 402/500 and loss: 1449.1439961194992\n",
      "Epoch 403/500 and loss: 1446.9671739339828\n",
      "Epoch 404/500 and loss: 1444.7985790967941\n",
      "Epoch 405/500 and loss: 1442.6381531953812\n",
      "Epoch 406/500 and loss: 1440.4858955144882\n",
      "Epoch 407/500 and loss: 1438.3417307138443\n",
      "Epoch 408/500 and loss: 1436.2055840492249\n",
      "Epoch 409/500 and loss: 1434.0774857997894\n",
      "Epoch 410/500 and loss: 1431.9573429822922\n",
      "Epoch 411/500 and loss: 1429.8452447652817\n",
      "Epoch 412/500 and loss: 1427.7413140535355\n",
      "Epoch 413/500 and loss: 1425.6455450057983\n",
      "Epoch 414/500 and loss: 1423.5579308271408\n",
      "Epoch 415/500 and loss: 1421.4784406423569\n",
      "Epoch 416/500 and loss: 1419.4069966077805\n",
      "Epoch 417/500 and loss: 1417.3436566591263\n",
      "Epoch 418/500 and loss: 1415.2883577346802\n",
      "Epoch 419/500 and loss: 1413.241050004959\n",
      "Epoch 420/500 and loss: 1411.2016590833664\n",
      "Epoch 421/500 and loss: 1409.170127749443\n",
      "Epoch 422/500 and loss: 1407.146419286728\n",
      "Epoch 423/500 and loss: 1405.1303857564926\n",
      "Epoch 424/500 and loss: 1403.1219676733017\n",
      "Epoch 425/500 and loss: 1401.1210489273071\n",
      "Epoch 426/500 and loss: 1399.1275157928467\n",
      "Epoch 427/500 and loss: 1397.1412887573242\n",
      "Epoch 428/500 and loss: 1395.1622587442398\n",
      "Epoch 429/500 and loss: 1393.1902955770493\n",
      "Epoch 430/500 and loss: 1391.2254067659378\n",
      "Epoch 431/500 and loss: 1389.2674651145935\n",
      "Epoch 432/500 and loss: 1387.3165419101715\n",
      "Epoch 433/500 and loss: 1385.3725373744965\n",
      "Epoch 434/500 and loss: 1383.4354211091995\n",
      "Epoch 435/500 and loss: 1381.5051053762436\n",
      "Epoch 436/500 and loss: 1379.58152115345\n",
      "Epoch 437/500 and loss: 1377.6644749641418\n",
      "Epoch 438/500 and loss: 1375.7539857625961\n",
      "Epoch 439/500 and loss: 1373.8498616218567\n",
      "Epoch 440/500 and loss: 1371.952029824257\n",
      "Epoch 441/500 and loss: 1370.060500741005\n",
      "Epoch 442/500 and loss: 1368.1752035617828\n",
      "Epoch 443/500 and loss: 1366.295998096466\n",
      "Epoch 444/500 and loss: 1364.4228156805038\n",
      "Epoch 445/500 and loss: 1362.5556420087814\n",
      "Epoch 446/500 and loss: 1360.6944152116776\n",
      "Epoch 447/500 and loss: 1358.839084982872\n",
      "Epoch 448/500 and loss: 1356.9897283315659\n",
      "Epoch 449/500 and loss: 1355.1462485790253\n",
      "Epoch 450/500 and loss: 1353.308672428131\n",
      "Epoch 451/500 and loss: 1351.476887702942\n",
      "Epoch 452/500 and loss: 1349.6508291959763\n",
      "Epoch 453/500 and loss: 1347.8303933143616\n",
      "Epoch 454/500 and loss: 1346.015666604042\n",
      "Epoch 455/500 and loss: 1344.2066278457642\n",
      "Epoch 456/500 and loss: 1342.4032423496246\n",
      "Epoch 457/500 and loss: 1340.6054439544678\n",
      "Epoch 458/500 and loss: 1338.813174366951\n",
      "Epoch 459/500 and loss: 1337.0264234542847\n",
      "Epoch 460/500 and loss: 1335.245050907135\n",
      "Epoch 461/500 and loss: 1333.4690383672714\n",
      "Epoch 462/500 and loss: 1331.6983423233032\n",
      "Epoch 463/500 and loss: 1329.9328343868256\n",
      "Epoch 464/500 and loss: 1328.172543644905\n",
      "Epoch 465/500 and loss: 1326.4174194335938\n",
      "Epoch 466/500 and loss: 1324.6674966812134\n",
      "Epoch 467/500 and loss: 1322.922779083252\n",
      "Epoch 468/500 and loss: 1321.1831784248352\n",
      "Epoch 469/500 and loss: 1319.4486725330353\n",
      "Epoch 470/500 and loss: 1317.7192459106445\n",
      "Epoch 471/500 and loss: 1315.9947805404663\n",
      "Epoch 472/500 and loss: 1314.2752029895782\n",
      "Epoch 473/500 and loss: 1312.5604815483093\n",
      "Epoch 474/500 and loss: 1310.850452542305\n",
      "Epoch 475/500 and loss: 1309.145124912262\n",
      "Epoch 476/500 and loss: 1307.4443747997284\n",
      "Epoch 477/500 and loss: 1305.7482203245163\n",
      "Epoch 478/500 and loss: 1304.057077050209\n",
      "Epoch 479/500 and loss: 1302.371099472046\n",
      "Epoch 480/500 and loss: 1300.6903669834137\n",
      "Epoch 481/500 and loss: 1299.014841556549\n",
      "Epoch 482/500 and loss: 1297.3445041179657\n",
      "Epoch 483/500 and loss: 1295.679356455803\n",
      "Epoch 484/500 and loss: 1294.0193980932236\n",
      "Epoch 485/500 and loss: 1292.3645230531693\n",
      "Epoch 486/500 and loss: 1290.7147316932678\n",
      "Epoch 487/500 and loss: 1289.0699323415756\n",
      "Epoch 488/500 and loss: 1287.4301363229752\n",
      "Epoch 489/500 and loss: 1285.795336484909\n",
      "Epoch 490/500 and loss: 1284.165471315384\n",
      "Epoch 491/500 and loss: 1282.5407668352127\n",
      "Epoch 492/500 and loss: 1280.921264886856\n",
      "Epoch 493/500 and loss: 1279.306937098503\n",
      "Epoch 494/500 and loss: 1277.6977528333664\n",
      "Epoch 495/500 and loss: 1276.09372484684\n",
      "Epoch 496/500 and loss: 1274.4948019981384\n",
      "Epoch 497/500 and loss: 1272.9009985923767\n",
      "Epoch 498/500 and loss: 1271.3122429847717\n",
      "Epoch 499/500 and loss: 1269.7285041809082\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for mix_data in sgns_data:\n",
    "        \n",
    "        target_word = mix_data[0][0]\n",
    "#         print(target_word)\n",
    "        context = [item[1] for item in mix_data]\n",
    "#         print(context)\n",
    "        \n",
    "        y_label = torch.unsqueeze(torch.tensor([item[2] for item in mix_data]), 1).float()\n",
    "        \n",
    "        target_idx = torch.tensor([hi_word_to_ix[target_word]])\n",
    "        context_idxs = torch.tensor([hi_word_to_ix[word] for word in context], dtype=torch.long)\n",
    "                                                                                      \n",
    "        network.zero_grad()\n",
    "        \n",
    "        scores = network(target_idx, context_idxs)\n",
    "        \n",
    "        loss = loss_fn(torch.transpose(scores,0,1), y_label)\n",
    "                                  \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                                  \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}/{} and loss: {}'.format(epoch, EPOCHS, total_loss))                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "383bf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(input_word, model, vocab_size, embedding_dim, n_neighbors=5):\n",
    "    \n",
    "    #Integer mapping\n",
    "    target_idx = torch.tensor([hi_word_to_ix[input_word]])\n",
    "    \n",
    "    #loading trained embedding\n",
    "    all_embeds = model.input_embedding.weight.view(1, vocab_size, embedding_dim)\n",
    "    \n",
    "    #input_word mapping\n",
    "    input_embed = model.input_embedding(torch.tensor([hi_word_to_ix[input_word]]))\n",
    "    \n",
    "    #cosine similarity\n",
    "    similarity_fn = nn.CosineSimilarity()\n",
    "    \n",
    "    scores = similarity_fn(input_embed, all_embeds)\n",
    "    \n",
    "    top_result = torch.topk(scores, n_neighbors+1)\n",
    "\n",
    "    pred_scores = [item for item in top_result.values.tolist()[0][1:]]\n",
    "    pred_indices = [item for item in top_result.indices.tolist()[0][1:]]\n",
    "    \n",
    "    preds = [(hi_ix_to_word[item], round(item1, 2)) for item, item1 in \n",
    "              zip(pred_indices[1:], pred_scores[1:])]\n",
    "    \n",
    "    return preds                                                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f196cb6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('इसकी', 0.13), ('होता', 0.13), ('लिए', 0.12), ('नियुक्ति', 0.12)]\n"
     ]
    }
   ],
   "source": [
    "# predictions Generation\n",
    "print(get_nearest_neighbors('झगड़ा', network, len(hi_vocab), EMBED_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b90d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2128ec8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e53796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8141b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
